import os
import io
import logging
import html2text
from typing import Dict, Any, List, Union, Optional

# Updated imports
import docx
import PyPDF2
import mammoth
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.language_models.chat_models import BaseChatModel

class DocumentAnalyzer:
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        """
        Enhanced initialization with more flexible model selection
        
        Args:
            api_key (str): OpenAI API key
            model (str): Specific model to use, defaults to gpt-4o-mini
        """
        self.llm = ChatOpenAI(
            model=model,
            api_key=api_key,
            temperature=0.5
        )
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = True
        self.html_converter.ignore_images = True
    
    def _extract_text_from_file(self, file_path: str) -> str:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            file_path (str): –ü—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É
        
        Returns:
            str: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        file_extension = os.path.splitext(file_path)[1].lower()
        
        try:
            with open(file_path, 'rb') as file:
                if file_extension == '.docx':
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º mammoth –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ DOCX
                    result = mammoth.convert_to_html(file)
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ plain text
                    text = self.html_converter.handle(result.value)
                    return text
                
                elif file_extension == '.pdf':
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyPDF2 –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ PDF
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    return text
                
                elif file_extension == '.txt':
                    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
                    return file.read().decode('utf-8')
                
                else:
                    raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_extension}")
        
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {file_path}: {e}")
            raise
    
    def analyze_document(self, document_path: str) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            document_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_text = self._extract_text_from_file(document_path)
            
            # –£—Å–µ—á–µ–Ω–∏–µ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å —Ç–æ–∫–µ–Ω–∞–º–∏
            max_tokens = 15000  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            document_text = document_text[:max_tokens]
            
            analysis_prompt = ChatPromptTemplate.from_messages([
                ("system", """
                –í—ã–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
                1. –í—ã—è–≤–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è
                2. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
                3. –°—Ä–∞–≤–Ω–∏—Ç–µ —Å —Ç–∏–ø–æ–≤—ã–º–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ —à–∞–±–ª–æ–Ω–∞–º–∏
                4. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –æ—Ü–µ–Ω–∫—É —Ä–∏—Å–∫–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                
                –û–±—Ä–∞—Ç–∏—Ç–µ –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞:
                - –î–æ–≥–æ–≤–æ—Ä–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
                - –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ª–∞–∑–µ–π–∫–∏
                - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–æ—Å—Å–∏–π—Å–∫–æ–º—É –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É
                - –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∏ –ø—Ä–∞–≤–æ–≤—ã–µ —Ä–∏—Å–∫–∏
                - –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                
                –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
                –ö–ª—é—á–µ–≤—ã–µ –æ–ª–æ–∂–µ–Ω–∏—è: [—Å–ø–∏—Å–æ–∫]
                –†–∏—Å–∫–∏: [—Å–ø–∏—Å–æ–∫]
                –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: [—Å–ø–∏—Å–æ–∫]
                –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –ø–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é: [—Å–ø–∏—Å–æ–∫]
                """),
                ("human", "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {document_text}")
            ])
            
            chain = analysis_prompt | self.llm | StrOutputParser()
            
            analysis_result = chain.invoke({"document_text": document_text})
            
            return {
                "success": True,
                "file_path": document_path,
                "analysis": self._parse_analysis(analysis_result)
            }
        
        except Exception as e:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_path": document_path
            }
    
    def _parse_analysis(self, response: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            response (str): –¢–µ–∫—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
        """
        sections = {
            "key_provisions": [],
            "risks": [],
            "recommendations": [],
            "compliance_notes": []
        }
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
        current_section = None
        for line in response.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π
            if '–∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è:' in line.lower():
                current_section = 'key_provisions'
                continue
            elif '—Ä–∏—Å–∫–∏:' in line.lower():
                current_section = 'risks'
                continue
            elif '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:' in line.lower():
                current_section = 'recommendations'
                continue
            elif '–ø—Ä–∏–º–µ—á–∞–Ω–∏—è –ø–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é:' in line.lower():
                current_section = 'compliance_notes'
                continue
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–µ–∫—Ü–∏–∏
            if current_section and line and not line.endswith(':'):
                # ÔøΩÔøΩ–µ –º–∞—Ä–∫–µ—Ä–æ–≤ —Å–ø–∏—Å–∫–∞ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                clean_line = line.lstrip('- ').lstrip('‚Ä¢ ').strip()
                if clean_line:
                    sections[current_section].append(clean_line)
        
        return sections

class EnhancedDocumentAnalyzer(DocumentAnalyzer):
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        super().__init__(api_key, model)
        
        # Add text_splitter as an attribute
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=4000, 
            chunk_overlap=200
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π risk_prompt –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        self.risk_prompt = ChatPromptTemplate.from_template(
            """–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô –∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:

            –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞: {text}

            –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
            1. –ù–∞–π–¥–∏—Ç–µ –ú–ò–ù–ò–ú–£–ú 3 —Ä–∏—Å–∫–∞
            2. –î–ª—è –ö–ê–ñ–î–û–ì–û —Ä–∏—Å–∫–∞ —É–∫–∞–∂–∏—Ç–µ:
               - –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞ (Low/Medium/High)
               - –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
               - –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è

            –ï—Å–ª–∏ —Ä–∏—Å–∫–æ–≤ –Ω–µ—Ç, –Ω–∞–ø–∏—à–∏—Ç–µ "–°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ".
            """
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π summary_prompt –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        self.summary_prompt = ChatPromptTemplate.from_template(
            """–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ, –¥–µ—Ç–∞–ª—å–Ω–æ–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞:

            –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞: {text}

            –°–¢–†–£–ö–¢–£–†–ê –†–ï–ó–Æ–ú–ï:
            1. –û–±—â–∞—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
               - –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
               - –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å
               - –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è

            2. –ö–ª—é—á–µ–≤—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏:
               - –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
               - –õ–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏
               - –°–º—ã—Å–ª–æ–≤—ã–µ –∞–∫—Ü–µ–Ω—Ç—ã

            3. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:
               - –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
               - –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏
               - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∏ –Ω–æ—Ä–º–∞–º

            4. –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –≤—ã–≤–æ–¥—ã:
               - –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
               - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
               - –í–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ—Ä–∞–±–æ—Ç–∫–∏

            –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
            - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å
            - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            - –õ–æ–≥–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            - –û–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫
            """
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π qa_prompt –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        self.qa_prompt = ChatPromptTemplate.from_template(
            """–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å:
            
            –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞: {context}
            
            –í–æ–ø—Ä–æ—Å: {question}
            
            –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
            - –û—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            - –ï—Å–ª–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
            - –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–∞–π—Ç–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è, —á–µ—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏—Ç–µ –ø—Ä–∏—á–∏–Ω—É
            """
        )
    
    def comprehensive_document_analysis(
        self, 
        document_path: str
    ) -> Dict[str, Any]:
        """
        Perform comprehensive document analysis
        """
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_text = self._extract_text_from_file(document_path)
            
            # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=4000, 
                chunk_overlap=200
            )
            texts = text_splitter.split_text(document_text)
            docs = [Document(page_content=text) for text in texts]
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤
            risks_analysis = self._analyze_document_risks(document_text)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏
            summary = self._generate_document_summary(docs)
            
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–≤–∏–∑–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            revised_document = self._revise_document(document_text)
            
            return {
                "success": True,
                "file_path": document_path,
                "risks": risks_analysis,
                "summary": summary,
                "revised_document": revised_document
            }
        
        except Exception as e:
            self.logger.error(f"Comprehensive analysis failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_path": document_path
            }
    
    def _analyze_document_risks(self, document_text: str) -> List[Dict[str, str]]:
        """
        Enhanced risk analysis with more robust processing
        
        Args:
            document_text (str): Full document text
        
        Returns:
            List of structured risks
        """
        try:
            risk_chain = (
                {"text": RunnablePassthrough()} 
                | self.risk_prompt 
                | self.llm 
                | StrOutputParser()
            )
            
            risks_text = risk_chain.invoke(document_text)
            
            # Add logging here
            print("Raw Risks Text:", risks_text)
            
            return self._parse_risks(risks_text)
        
        except Exception as e:
            self.logger.error(f"Risk analysis failed: {e}")
            return []
    
    def _generate_document_summary(self, docs: List[Document]) -> str:
        """
        More robust summary generation
        
        Args:
            docs (List[Document]): Document chunks
        
        Returns:
            Summarized text
        """
        try:
            # Combine all document chunks
            text = " ".join([doc.page_content for doc in docs])
            
            # Use a more robust summary chain
            summary_chain = (
                {"text": RunnablePassthrough()} 
                | self.summary_prompt 
                | self.llm 
                | StrOutputParser()
            )
            
            summary = summary_chain.invoke(text)
            
            # Ensure non-empty summary
            return summary if summary.strip() else "Unable to generate summary"
        
        except Exception as e:
            self.logger.error(f"Summary generation failed: {e}")
            return "Unable to generate summary"
    
    def _revise_document(self, document_text: str) -> Optional[str]:
        """
        Placeholder for document revision logic
        
        Args:
            document_text (str): Original document text
        
        Returns:
            Potentially revised document text
        """
        # Future implementation for document revision
        # Could involve legal language refinement, risk mitigation suggestions
        return None
    
    def _parse_risks(self, risks_text: str) -> List[Dict[str, str]]:
        print("Parsing Risks Text:", risks_text)  # Debug print
        
        risks = []
        if not risks_text or len(risks_text.strip()) < 10:
            print("No meaningful risks text found")
            return risks

        # More flexible parsing
        risk_sections = risks_text.split('\n\n')
        for section in risk_sections:
            if any(keyword in section.lower() for keyword in ['low', 'medium', 'high']):
                risk = {
                    'severity': 'Unknown',
                    'description': section.strip(),
                    'mitigation': ''
                }
                risks.append(risk)

        print(f"Parsed {len(risks)} risks")
        return risks

def analyze_legal_document(api_key: str, document_path: str) -> Dict[str, Any]:
    """
    –§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    Args:
        api_key (str): API-–∫–ª—é—á OpenAI
        document_path (str): –ü—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    analyzer = DocumentAnalyzer(api_key)
    return analyzer.analyze_document(document_path)

def analyze_comprehensive_document(
    api_key: str, 
    document_path: str
) -> Dict[str, Any]:
    """
    Wrapper function for comprehensive document analysis
    
    Args:
        api_key (str): OpenAI API key
        document_path (str): Path to document
    
    Returns:
        Comprehensive analysis results
    """
    analyzer = EnhancedDocumentAnalyzer(api_key)
    return analyzer.comprehensive_document_analysis(document_path)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä—è–º–æ–º –∑–∞–ø—É—Å–∫–µ —Å–∫—Ä–∏–ø—Ç–∞
# if __name__ == "__main__":
#     import os
#     import json
    
#     # –ü–æ–ª—É—á–µ–Ω–∏–µ API-–∫–ª—é—á–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
#     api_key = os.getenv("OPENAI_API_KEY")
    
#     # –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É
#     sample_document = "/home/dukhanin/fic/docs/–ü—Ä–∏–∫–∞–∑ –æ –Ω–∞—á–∞–ª–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.docx"
    
#     try:
#         # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
#         analyzer = EnhancedDocumentAnalyzer(api_key)
        
#         print("=" * 50)
#         print(" Comprehensive Document Analysis")
#         print("=" * 50)
        
#         # 1. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
#         comprehensive_result = analyzer.comprehensive_document_analysis(
#             sample_document
#         )
        
#         # 2. –û—Ç–¥–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤
#         print("\nüìä –†–∏—Å–∫–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:")
#         risks = analyzer._analyze_document_risks(
#             analyzer._extract_text_from_file(sample_document)
#         )
#         for risk in risks:
#             print(f"- {risk}")
        
#         # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏
#         print("\nüìù –°–∞–º–º–∞—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=4000, 
#             chunk_overlap=200
#         )
#         document_text = analyzer._extract_text_from_file(sample_document)
#         texts = text_splitter.split_text(document_text)
#         docs = [Document(page_content=text) for text in texts]
        
#         summary = analyzer._generate_document_summary(docs)
#         print(summary)
        
#         # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –ø–æ–ª–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
#         print("\nüèÅ –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
#         print(json.dumps(comprehensive_result, indent=2, ensure_ascii=False))
    
#     except Exception as e:
#         print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")